"""ãƒ‹ãƒ¥ãƒ¼ã‚¹é–¢é€£ãƒ«ãƒ¼ãƒˆ"""
import uuid
from datetime import datetime

from fastapi import APIRouter, Request, HTTPException, Form, Header
from fastapi.responses import HTMLResponse, RedirectResponse, Response
from fastapi.templating import Jinja2Templates
from pathlib import Path

from app.config import settings, is_rss_and_ai_disabled
from app.services.news_aggregator import NewsAggregator
from app.services.rss_service import NewsItem, sanitize_display_text
from app.services.article_cache import save_article
from app.services.ai_batch_service import generate_all_explanations
from app.services.ai_service import (
    explain_article_with_ai,
    get_image_url,
    PERSONAS,
)

router = APIRouter()
templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent.parent / "templates"))


@router.get("/robots.txt")
async def robots_txt(request: Request):
    """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å‘ã‘ robots.txt"""
    site_url = _get_site_url(request)
    body = f"User-agent: *\nAllow: /\nDisallow: /admin\nDisallow: /confirm\n\nSitemap: {site_url}/sitemap.xml\n"
    return Response(content=body, media_type="text/plain; charset=utf-8")


@router.get("/sitemap.xml")
async def sitemap_xml(request: Request):
    """SEOç”¨ sitemap.xml"""
    from app.services.article_cache import load_all
    from app.services.explanation_cache import get_cached_article_ids

    site_url = _get_site_url(request)
    all_articles = load_all()
    processed = get_cached_article_ids()
    articles = [a for a in all_articles if a.id in processed]
    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
        f"  <url><loc>{site_url}/</loc><changefreq>hourly</changefreq><priority>1.0</priority></url>",
    ]
    for a in articles[:5000]:
        lines.append(f"  <url><loc>{site_url}/topic/{a.id}</loc><changefreq>weekly</changefreq><priority>0.8</priority></url>")
    lines.append("</urlset>")
    return Response(content="\n".join(lines), media_type="application/xml; charset=utf-8")


@router.get("/", response_class=HTMLResponse)
async def index(request: Request, page: int = 1, keyword: str = ""):
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥è¡¨ç¤ºãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰ã€‚keyword æŒ‡å®šæ™‚ã¯é–¢é€£è¨˜äº‹ã®ã¿è¡¨ç¤º"""
    from app.services.news_aggregator import CATEGORY_ORDER, ITEMS_PER_PAGE
    keyword = (keyword or "").strip()
    if keyword:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼šå…¨è¨˜äº‹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ãƒ»è¦ç´„ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ã ã‘æ®‹ã—ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‹ã‚¸ãƒ£ãƒ³ãƒ«å†é›†è¨ˆ
        all_news = NewsAggregator.get_news()
        kw_lower = keyword.lower()
        filtered = [a for a in all_news if kw_lower in (a.title or "").lower() or kw_lower in (a.summary or "").lower()]
        per_page = ITEMS_PER_PAGE
        total = len(filtered)
        total_pages = max(1, (total + per_page - 1) // per_page)
        page = max(1, min(page, total_pages))
        start = (page - 1) * per_page
        page_items = filtered[start : start + per_page]
        by_cat: dict[str, list] = {}
        for a in page_items:
            by_cat.setdefault(a.category, []).append(a)
        news_by_category = [(c, by_cat.get(c, [])) for c in CATEGORY_ORDER]
        pagination = {"page": page, "per_page": per_page, "total": total, "total_pages": total_pages, "has_prev": page > 1, "has_next": page < total_pages}
    else:
        news_by_category, pagination = NewsAggregator.get_news_by_category(page=page)
    trends = NewsAggregator.get_trends()
    added_one = None
    for _, items in news_by_category:
        for item in items:
            _ensure_japanese(item)
            if not item.image_url:
                item.image_url = get_image_url(item.id, 400, 225)
            elif item.image_url and not item.image_url.startswith("http"):
                item.image_url = get_image_url(item.image_url, 400, 225)
    site_url = _get_site_url(request)
    og_image = "https://picsum.photos/1200/630"
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "news_by_category": news_by_category,
            "trends": trends,
            "pagination": pagination,
            "added_one": added_one,
            "site_url": site_url,
            "og_image": og_image,
            "search_keyword": keyword,
        }
    )


@router.get("/api/news/page")
async def api_news_page(page: int = 1, keyword: str = ""):
    """ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ç”¨ï¼šãƒšãƒ¼ã‚¸ã®è¨˜äº‹ã‚«ãƒ¼ãƒ‰HTMLã‚’è¿”ã™"""
    from app.services.news_aggregator import CATEGORY_ORDER, ITEMS_PER_PAGE
    keyword = (keyword or "").strip()
    if keyword:
        all_news = NewsAggregator.get_news()
        kw_lower = keyword.lower()
        news = [a for a in all_news if kw_lower in (a.title or "").lower() or kw_lower in (a.summary or "").lower()]
    else:
        news = NewsAggregator.get_news()
    total = len(news)
    per_page = ITEMS_PER_PAGE
    total_pages = max(1, (total + per_page - 1) // per_page)
    page = max(1, min(page, total_pages))
    start = (page - 1) * per_page
    items = news[start : start + per_page]
    for item in items:
        _ensure_japanese(item)
        if not item.image_url:
            item.image_url = get_image_url(item.id, 400, 225)
        elif not item.image_url.startswith("http"):
            item.image_url = get_image_url(item.image_url, 400, 225)
    import html as html_mod
    cards_html = ""
    for item in items:
        pub = item.published.strftime('%m/%d %H:%M') if item.published else ''
        title_safe = html_mod.escape(item.title or "")
        summary_safe = html_mod.escape((item.summary or "")[:80])
        source_safe = html_mod.escape(item.source or "")
        cat_safe = html_mod.escape(item.category or "")
        cards_html += f'''<article class="news-card animate-fade-in" data-category="{cat_safe}">
<a href="/topic/{item.id}" class="news-card-link">
<div class="news-card-image"><img src="{item.image_url or 'https://picsum.photos/400/225'}" alt="{title_safe}" loading="lazy"><span class="news-card-category">{cat_safe}</span></div>
<div class="news-card-body">
<div class="news-card-meta"><span class="news-card-time">ğŸ•’ {pub}</span><span class="news-card-source">{source_safe}</span></div>
<h3 class="news-title">{title_safe}</h3>
<p class="news-summary-line">ğŸ‘€ {summary_safe}...</p>
<div class="news-card-footer"><span class="news-card-ai">âœ AIãŒè§£èª¬</span><span class="news-badge">AIè§£èª¬</span></div>
</div></a></article>'''
    return {"html": cards_html, "page": page, "total_pages": total_pages}


@router.get("/trend", response_class=HTMLResponse)
async def trend_page(request: Request):
    """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒšãƒ¼ã‚¸ï¼šã‚¹ã‚³ã‚¢ãŒé«˜ã„è¨˜äº‹"""
    all_news = NewsAggregator.get_news()
    trends = NewsAggregator.get_trends()
    trend_keywords = [t.keyword for t in trends]
    scored = []
    for item in all_news:
        text = f"{item.title} {item.summary}"
        score = sum(1 for kw in trend_keywords if kw.lower() in text.lower())
        scored.append((score, item))
    scored.sort(key=lambda x: x[0], reverse=True)
    top_articles = [item for _, item in scored[:30]]
    for item in top_articles:
        _ensure_japanese(item)
        if not item.image_url:
            item.image_url = get_image_url(item.id, 400, 225)
        elif not item.image_url.startswith("http"):
            item.image_url = get_image_url(item.image_url, 400, 225)
    return templates.TemplateResponse("trend.html", {"request": request, "articles": top_articles, "trends": trends})


@router.get("/ai", response_class=HTMLResponse)
async def ai_page(request: Request):
    """AIãƒšãƒ¼ã‚¸ï¼šãŠã™ã™ã‚ãƒ»æ˜¨æ—¥ã®ãƒ¡ãƒ¢ãƒ»äººæ ¼ã‚³ãƒ¡ãƒ³ãƒˆ"""
    all_news = NewsAggregator.get_news()
    recommended = all_news[:6]
    for item in recommended:
        _ensure_japanese(item)
        if not item.image_url:
            item.image_url = get_image_url(item.id, 400, 225)
        elif not item.image_url.startswith("http"):
            item.image_url = get_image_url(item.image_url, 400, 225)
    ai_memo = None
    ai_personas = []
    try:
        from app.services.ai_daily import get_daily_ai_content
        daily = get_daily_ai_content()
        if daily:
            ai_memo = daily.get("memo", "")
            ai_personas = daily.get("persona_comments", [])
    except Exception:
        pass
    return templates.TemplateResponse("ai.html", {"request": request, "recommended": recommended, "ai_memo": ai_memo, "ai_personas": ai_personas})


@router.get("/search", response_class=HTMLResponse)
async def search_page(request: Request, q: str = ""):
    """æ¢ã™ãƒšãƒ¼ã‚¸"""
    q = (q or "").strip()
    results = []
    if q:
        all_news = NewsAggregator.get_news()
        ql = q.lower()
        results = [a for a in all_news if ql in (a.title or "").lower() or ql in (a.summary or "").lower()][:50]
        for item in results:
            _ensure_japanese(item)
            if not item.image_url:
                item.image_url = get_image_url(item.id, 400, 225)
            elif not item.image_url.startswith("http"):
                item.image_url = get_image_url(item.image_url, 400, 225)
    return templates.TemplateResponse("search.html", {"request": request, "query": q, "results": results})


@router.get("/saved", response_class=HTMLResponse)
async def saved_page(request: Request):
    """ä¿å­˜æ¸ˆã¿è¨˜äº‹ãƒšãƒ¼ã‚¸ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹ï¼‰"""
    return templates.TemplateResponse("saved.html", {"request": request})


# ç¢ºèªç”¨ãƒšãƒ¼ã‚¸ã§è¡¨ç¤ºã™ã‚‹ç›´è¿‘ã®è¨˜äº‹æ•°
CONFIRM_PAGE_ARTICLE_LIMIT = 20


@router.get("/confirm", response_class=HTMLResponse)
async def confirm_page(request: Request):
    """ç¢ºèªç”¨ãƒšãƒ¼ã‚¸ï¼šæœ€æ–°è¨˜äº‹ã®1ä»¶å–ã‚Šè¾¼ã¿ã¨ã€è¨˜äº‹ã®å‰Šé™¤"""
    news = NewsAggregator.get_news()[:CONFIRM_PAGE_ARTICLE_LIMIT]
    for item in news:
        if not item.image_url:
            item.image_url = get_image_url(item.id, 400, 225)
        elif item.image_url and not item.image_url.startswith("http"):
            item.image_url = get_image_url(item.image_url, 400, 225)
    return templates.TemplateResponse(
        "confirm.html",
        {"request": request, "recent_articles": news}
    )


def _ensure_japanese(item):
    """ä¿å­˜æ™‚ã«æ—¥æœ¬èªåŒ–æ¸ˆã¿ã®ãŸã‚ã€è¡¨ç¤ºå´ã§ã¯ä½•ã‚‚ã—ãªã„"""
    pass


def _get_site_url(request: Request) -> str:
    """ã‚µã‚¤ãƒˆã®çµ¶å¯¾URLï¼ˆæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—ï¼‰"""
    base = getattr(settings, "SITE_URL", "").strip().rstrip("/")
    if base:
        return base
    return str(request.base_url).rstrip("/")


def _meta_description_qa(title: str, summary: str | None, max_len: int = 160) -> str:
    """è³ªå•ï¼‹è§£ç­”å‹ã®meta descriptionï¼ˆSEOå‘ã‘ãƒ»ã€Œãªãœã€ã€Œç†ç”±ã€ã€Œä½•ã€ã‚’å…¥ã‚Œã‚‹ï¼‰"""
    t = (title or "").strip()
    s = (summary or "").replace("\n", " ").strip()[:200]
    if not t:
        return (s[: max_len - 3] + "...") if len(s) > max_len else s
    # SEOç”¨ã«ã€Œãªãœã€ã€Œç†ç”±ã€ã€Œä½•ã€ã‚’å«ã‚€ç–‘å•å½¢ã«ã™ã‚‹
    if "ãªãœ" in t or "ç†ç”±" in t:
        question = f"{t}ã®ç†ç”±ã¨ã¯ï¼Ÿ"
    elif "ä½•" in t or "ã¨ã¯" in t:
        question = f"{t}ã‚’è§£èª¬"
    else:
        question = f"{t}ã¨ã¯ä½•ï¼Ÿãªãœèµ·ããŸï¼Ÿ"
    if not s:
        return question[:max_len]
    answer = s[: max_len - len(question) - 4] + "..." if len(s) > max_len - len(question) - 2 else s
    return f"{question} {answer}"[:max_len]


def _build_short_summary(blocks: list, fallback_summary: str | None) -> str:
    """ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ã€Œ1åˆ†ã§ç†è§£ã€ç”¨ã®è¦ç‚¹ã¾ã¨ã‚HTMLã‚’ç”Ÿæˆ"""
    import html as _html
    if not blocks:
        s = (fallback_summary or "").strip()
        return f'<p class="article-text">{_html.escape(s)}</p>' if s else ""

    points = []
    is_nav = blocks and blocks[0].get("type") == "navigator_section"
    if is_nav:
        nav_labels = {"facts": "ğŸ“Œ äº‹å®Ÿ", "background": "ğŸ“– èƒŒæ™¯", "impact": "ğŸ¯ å½±éŸ¿", "prediction": "ğŸ”® äºˆæ¸¬", "caution": "âš  æ³¨æ„"}
        for b in blocks:
            sec = b.get("section", "")
            content = (b.get("content") or "").strip()
            if sec in nav_labels and content:
                label = nav_labels[sec]
                safe = _html.escape(content[:300]).replace("\n", "<br>")
                points.append(f'<div class="short-point"><span class="short-point-label">{label}</span><p>{safe}</p></div>')
    else:
        text_parts = []
        for b in blocks:
            if b.get("type") == "text":
                text_parts.append((b.get("content") or "").strip())
        combined = "\n\n".join(text_parts)
        paras = [p.strip() for p in combined.split("\n\n") if p.strip()]
        for p in paras[:3]:
            safe = _html.escape(p[:180]).replace("\n", "<br>")
            points.append(f'<div class="short-point"><p>{safe}{"..." if len(p) > 180 else ""}</p></div>')
    return "\n".join(points) if points else f'<p class="article-text">{_html.escape((fallback_summary or "")[:500])}</p>'


def _blocks_to_html(blocks: list) -> str:
    """ãƒ–ãƒ­ãƒƒã‚¯ã‚’HTMLã«å¤‰æ›ã€‚æœ¬æ–‡ã®ã¿è¡¨ç¤ºã€‚ãƒŸãƒ‰ãƒ«ãƒãƒ³è§£èª¬ã¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å¹ãå‡ºã—ç”¨ã® JSON ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦åŸ‹ã‚è¾¼ã‚€"""
    if not blocks:
        return ""
    import html as _h
    import json as _json
    text_parts: list[str] = []
    float_items: list[dict] = []
    is_navigator = blocks and blocks[0].get("type") == "navigator_section"
    nav_labels = {"facts": "ãƒ‹ãƒ¥ãƒ¼ã‚¹", "background": "èƒŒæ™¯", "impact": "å½±éŸ¿ç¯„å›²", "prediction": "äºˆæ¸¬", "caution": "æ³¨æ„"}
    if is_navigator:
        for b in blocks:
            if b.get("type") != "navigator_section" or not b.get("section"):
                continue
            body = (b.get("content") or "").strip()
            if not body:
                continue
            if b.get("section") == "facts":
                for p in body.split("\n\n"):
                    p = p.strip()
                    if p:
                        text_parts.append(_h.escape(p).replace("\n", "<br>"))
            else:
                label = nav_labels.get(b["section"], b["section"])
                float_items.append({"label": label, "body": _h.escape(body).replace("\n", "<br>")})
    else:
        for b in blocks:
            if b.get("type") == "text":
                for p in (b.get("content") or "").strip().split("\n\n"):
                    p = p.strip()
                    if p:
                        text_parts.append(_h.escape(p).replace("\n", "<br>"))
            elif b.get("type") == "explain":
                c = (b.get("content") or "").strip()
                if c:
                    float_items.append({"label": "ãƒŸãƒ‰ãƒ«ãƒãƒ³", "body": _h.escape(c).replace("\n", "<br>")})
    out = ['<div class="article-readflow">']
    for i, p in enumerate(text_parts):
        out.append(f'<p class="article-text" data-para="{i}">{p}</p>')
    if float_items:
        out.append(f'<script type="application/json" class="midorman-float-data">{_json.dumps(float_items, ensure_ascii=False)}</script>')
    out.append("</div>")
    return "".join(out)


@router.get("/topic/{topic_id}", response_class=HTMLResponse)
async def topic_detail(request: Request, topic_id: str):
    """ãƒˆãƒ”ãƒƒã‚¯è©³ç´°ï¼ˆURL: /topic/â—‹â—‹ï¼‰ãƒ»AIè§£èª¬ãƒ»SEOå‘ã‘æœ¬æ–‡"""
    from app.services.explanation_cache import get_cached

    item = NewsAggregator.get_article(topic_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    _ensure_japanese(item)
    image_url = item.image_url or get_image_url(item.id, 800, 450)
    if image_url and not image_url.startswith("http"):
        image_url = get_image_url(image_url, 800, 450)
    site_url = _get_site_url(request)
    article_url = f"{site_url}/topic/{topic_id}"
    og_image = image_url if (image_url or "").startswith("http") else f"{site_url}{image_url}" if image_url else ""
    if not og_image:
        og_image = get_image_url(item.id, 1200, 630)
    cached = get_cached(topic_id)
    blocks = _sanitize_blocks(cached["blocks"]) if cached and cached.get("blocks") else []
    personas_data = cached.get("personas", []) if cached else []
    quick_understand = cached.get("quick_understand") if cached else None
    vote_data = cached.get("vote_data") if cached else None
    body_html = _blocks_to_html(blocks) if blocks else ""
    short_summary = _build_short_summary(blocks, item.summary)
    meta_desc = _meta_description_qa(item.title, item.summary)

    all_news = NewsAggregator.get_news()
    next_article = prev_article = None
    for i, a in enumerate(all_news):
        if a.id == topic_id:
            if i + 1 < len(all_news):
                next_article = all_news[i + 1]
            if i > 0:
                prev_article = all_news[i - 1]
            break

    related = [a for a in all_news if a.category == item.category and a.id != topic_id][:4]
    import random as _rnd
    other_cat = [a for a in all_news if a.category != item.category and a.id != topic_id]
    ai_recommended = _rnd.sample(other_cat, min(3, len(other_cat))) if other_cat else []

    return templates.TemplateResponse(
        "article.html",
        {
            "request": request,
            "article": item,
            "image_url": image_url,
            "personas": PERSONAS,
            "site_url": site_url,
            "article_url": article_url,
            "og_image": og_image,
            "blocks": blocks,
            "personas_data": personas_data,
            "body_html": body_html,
            "short_summary": short_summary,
            "meta_description": meta_desc,
            "next_article": next_article,
            "prev_article": prev_article,
            "related_articles": related,
            "same_category_articles": related,
            "ai_recommended": ai_recommended,
            "quick_understand": quick_understand,
            "vote_data": vote_data,
        }
    )


@router.get("/article/{article_id}", response_class=HTMLResponse)
async def article_detail(request: Request, article_id: str):
    """æ—§URL: /topic/ ã¸ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url=f"/topic/{article_id}", status_code=301)


def _sanitize_blocks(blocks: list) -> list:
    """ãƒ–ãƒ­ãƒƒã‚¯ã®contentã‹ã‚‰HTMLæ–­ç‰‡ã‚’é™¤å»ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿æ‚ªãƒ‡ãƒ¼ã‚¿å¯¾ç­–ï¼‰"""
    out = []
    for b in (blocks or []):
        if isinstance(b, dict) and "content" in b and b.get("content"):
            b = {**b, "content": sanitize_display_text(str(b["content"]))}
        out.append(b)
    return out


@router.get("/api/article/{article_id}/explain")
async def api_explain_article(article_id: str):
    """è¨˜äº‹ã®AIè§£èª¬ã‚’å–å¾—ï¼ˆå¾“æ¥å½¢å¼ãƒ»ã‚µã‚¤ãƒ‰ãƒ‘ãƒãƒ«ç”¨ï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    content = f"{item.title}\n\n{item.summary}"
    explanation = explain_article_with_ai(item.title, content)
    return {"explanation": explanation}


@router.get("/api/article/{article_id}/explain-inline")
async def api_explain_inline(article_id: str):
    """è¨˜äº‹æœ¬æ–‡ã¨è§£èª¬ãŒäº¤äº’ã«å…¥ã£ãŸæ§‹é€ ã§å–å¾—ï¼ˆå¾“æ¥APIãƒ»äº’æ›ç”¨ï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    data = generate_all_explanations(article_id, item.title, f"{item.title}\n\n{item.summary}")
    return {"blocks": _sanitize_blocks(data["blocks"])}


@router.get("/api/article/{article_id}/explanations")
async def api_all_explanations(article_id: str):
    """ãƒŸãƒ‰ãƒ«ãƒãƒ³è§£èª¬ï¼‹5äººæ ¼ã®æ„è¦‹ã‚’ä¸€æ‹¬å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆãƒ»ä¸€æ‹¬ç”Ÿæˆï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    content = f"{item.title}\n\n{item.summary}"
    data = generate_all_explanations(article_id, item.title, content)
    return {"blocks": _sanitize_blocks(data["blocks"]), "personas": data["personas"]}


@router.get("/api/article/{article_id}/opinion/{persona_id}")
async def api_persona_opinion(article_id: str, persona_id: int):
    """5äººæ ¼ã®AIã®ã†ã¡1äººã®æ„è¦‹ã‚’å–å¾—ï¼ˆå¾“æ¥APIãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµŒç”±ï¼‰"""
    if persona_id < 0 or persona_id >= len(PERSONAS):
        raise HTTPException(status_code=404, detail="äººæ ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    data = generate_all_explanations(article_id, item.title, f"{item.title}\n\n{item.summary}")
    opinion = data["personas"][persona_id] if persona_id < len(data["personas"]) else ""
    return {"persona": PERSONAS[persona_id], "opinion": opinion}


@router.get("/api/status")
async def api_status():
    """çŠ¶æ…‹ç¢ºèªï¼ˆè¨˜äº‹æ•°ãƒ»DBãƒ‘ã‚¹ç­‰ï¼‰"""
    from app.services.explanation_cache import get_cached_article_ids
    from app.services.article_cache import load_all

    processed = get_cached_article_ids()
    all_arts = load_all()
    displayable = [a for a in all_arts if a.id in processed]
    try:
        from app.config import settings
        has_key = bool(getattr(settings, "OPENAI_API_KEY", ""))
    except Exception:
        has_key = False

    return {
        "articles_in_db": len(all_arts),
        "ai_processed": len(processed),
        "displayable": len(displayable),
        "openai_key_set": has_key,
    }


@router.get("/api/news/refresh")
async def api_refresh_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ‰‹å‹•æ›´æ–°ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã€å³å¿œç­”ï¼‰"""
    import threading

    def _refresh():
        NewsAggregator.get_trends(force_refresh=True)  # ãƒˆãƒ¬ãƒ³ãƒ‰ã¯é€Ÿã„
        if not is_rss_and_ai_disabled():
            NewsAggregator.get_news(force_refresh=True)  # è¨˜äº‹ã¯AIå‡¦ç†ã§é…ã„ï¼ˆç„¡åŠ¹æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰

    threading.Thread(target=_refresh, daemon=True).start()
    msg = "æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã—ãŸ" if not is_rss_and_ai_disabled() else "ãƒˆãƒ¬ãƒ³ãƒ‰ã®ã¿æ›´æ–°ã—ã¾ã—ãŸï¼ˆRSSãƒ»AIã¯ã“ã®ç’°å¢ƒã§ã¯ç„¡åŠ¹ã§ã™ï¼‰"
    return {"status": "ok", "message": msg}


def _is_admin(request: Request, x_admin_secret: str | None = Header(None, alias="X-Admin-Secret")) -> bool:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¾ãŸã¯ X-Admin-Secret ãƒ˜ãƒƒãƒ€ã§ç®¡ç†è€…ã‹åˆ¤å®š"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        return False
    if x_admin_secret and x_admin_secret.strip() == settings.ADMIN_SECRET:
        return True
    return request.session.get("admin") is True


@router.get("/admin/login", response_class=HTMLResponse)
async def admin_login_page(request: Request):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ è¡¨ç¤º"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        return templates.TemplateResponse("admin_login.html", {"request": request, "error": "ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ï¼ˆADMIN_SECRET æœªè¨­å®šï¼‰"})
    if request.session.get("admin"):
        return RedirectResponse(url="/admin", status_code=302)
    err = "ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“" if request.query_params.get("error") == "invalid" else None
    return templates.TemplateResponse("admin_login.html", {"request": request, "error": err})


@router.post("/admin/login", response_class=RedirectResponse)
async def admin_login_submit(request: Request, secret: str = Form(...)):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        raise HTTPException(status_code=403, detail="ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
    if secret.strip() != settings.ADMIN_SECRET:
        return RedirectResponse(url="/admin/login?error=invalid", status_code=302)
    request.session["admin"] = True
    return RedirectResponse(url="/admin", status_code=302)


@router.get("/admin/logout", response_class=RedirectResponse)
async def admin_logout(request: Request):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"""
    request.session.pop("admin", None)
    return RedirectResponse(url="/", status_code=302)


@router.get("/admin", response_class=HTMLResponse)
async def admin_manual_article_page(request: Request):
    """æ‰‹å‹•è¨˜äº‹è¿½åŠ ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆç®¡ç†è€…ã®ã¿ï¼‰"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        raise HTTPException(status_code=403, detail="ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
    if not request.session.get("admin"):
        return RedirectResponse(url="/admin/login", status_code=302)
    return templates.TemplateResponse("admin_manual_article.html", {"request": request})


def _do_create_manual_article_sync(title: str, summary: str, link: str = "", source: str = "ç·¨é›†éƒ¨") -> dict:
    """æ‰‹å‹•è¨˜äº‹ã‚’AIã§ç”Ÿæˆã—ã¦ä¿å­˜ï¼ˆåŒæœŸãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œç”¨ï¼‰ã€‚generate_all_explanations å†…ã§ save_cache æ¸ˆã¿"""
    article_id = "manual-" + uuid.uuid4().hex[:16]
    content = sanitize_display_text(f"{title}\n\n{summary}")[:20000]
    data = generate_all_explanations(article_id, title, content)
    blocks = data.get("blocks", [])
    if not blocks:
        return {"status": "error", "article_id": None, "message": "AIã«ã‚ˆã‚‹è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"}
    item = NewsItem(
        id=article_id,
        title=title,
        link=link or "#",
        summary=summary[:4000],
        published=datetime.now(),
        source=source or "ç·¨é›†éƒ¨",
        category="ç·åˆ",
        image_url=None,
    )
    if not save_article(item):
        return {"status": "error", "article_id": None, "message": "è¨˜äº‹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ"}
    NewsAggregator.get_news(force_refresh=not is_rss_and_ai_disabled())
    return {"status": "ok", "article_id": article_id}


@router.post("/api/admin/article/manual")
async def api_admin_article_manual(
    request: Request,
    x_admin_secret: str | None = Header(None, alias="X-Admin-Secret"),
):
    """æ‰‹å‹•ã§æ¦‚è¦ã‚’é€ã‚Šã€AIãŒç†è§£ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼å½¢å¼ã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦è¿½åŠ ï¼ˆç®¡ç†è€…ã®ã¿ï¼‰"""
    if not _is_admin(request, x_admin_secret):
        raise HTTPException(status_code=403, detail="ç®¡ç†è€…ã®ã¿åˆ©ç”¨ã§ãã¾ã™")
    if is_rss_and_ai_disabled():
        raise HTTPException(status_code=503, detail="ã“ã®ç’°å¢ƒã§ã¯RSSå–å¾—ãƒ»AIè¦ç´„ã¯ç„¡åŠ¹ã§ã™ã€‚æ‰‹å‹•è¨˜äº‹è¿½åŠ ã¯ãƒ­ãƒ¼ã‚«ãƒ«ç­‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    try:
        body = await request.json()
    except Exception:
        raise HTTPException(status_code=400, detail="JSON ã§ title, summary ã‚’é€ã£ã¦ãã ã•ã„")
    title = (body.get("title") or "").strip()
    summary = (body.get("summary") or "").strip()
    if not title or not summary:
        raise HTTPException(status_code=400, detail="ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã¯å¿…é ˆã§ã™")
    link = (body.get("link") or "").strip()
    source = (body.get("source") or "ç·¨é›†éƒ¨").strip() or "ç·¨é›†éƒ¨"
    import asyncio
    loop = asyncio.get_event_loop()
    result = await asyncio.wait_for(
        loop.run_in_executor(
            None,
            lambda: _do_create_manual_article_sync(title, summary, link, source),
        ),
        timeout=180.0,
    )
    return result


@router.post("/api/admin/article/{article_id}/clear-cache")
async def api_clear_article_cache(article_id: str):
    """è¨˜äº‹ã®è§£èª¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼ˆå†ç”Ÿæˆã•ã›ã‚‹ï¼‰"""
    from app.services.explanation_cache import delete_cache
    deleted = delete_cache(article_id)
    return {"status": "ok", "deleted": deleted, "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚æ¬¡å›ã‚¢ã‚¯ã‚»ã‚¹ã§å†ç”Ÿæˆã•ã‚Œã¾ã™ã€‚"}


@router.post("/api/admin/article/{article_id}/delete")
async def api_delete_article(article_id: str):
    """è¨˜äº‹ã‚’å®Œå…¨ã«å‰Šé™¤ï¼ˆè§£èª¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‹è¨˜äº‹DBã‹ã‚‰å‰Šé™¤ï¼‰"""
    from app.services.explanation_cache import delete_cache
    from app.services.article_cache import delete_article as delete_article_from_db
    deleted_cache = delete_cache(article_id)
    deleted_article = delete_article_from_db(article_id)
    NewsAggregator.get_news(force_refresh=not is_rss_and_ai_disabled())
    return {
        "status": "ok",
        "deleted": deleted_cache or deleted_article,
        "message": "è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚",
    }


@router.get("/api/admin/seed-articles")
async def api_seed_articles():
    """RSSã‹ã‚‰ãƒŸãƒ‰ãƒ«ãƒãƒ³AIè§£èª¬ä»˜ãã§è¨˜äº‹ã‚’æŠ•å…¥ï¼ˆæ–°ç€5ä»¶ï¼‰"""
    if is_rss_and_ai_disabled():
        raise HTTPException(status_code=503, detail="ã“ã®ç’°å¢ƒã§ã¯RSSå–å¾—ãƒ»AIè¦ç´„ã¯ç„¡åŠ¹ã§ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ç­‰ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_new_rss_articles
    from app.services.explanation_cache import get_cached_article_ids

    news = fetch_rss_news()
    added = process_new_rss_articles(news, max_per_run=5)
    NewsAggregator.get_news(force_refresh=True)
    total = len(get_cached_article_ids())
    return {"status": "ok", "added": added, "total": total}


def _do_seed_one_sync():
    """RSSå–å¾—â†’1ä»¶ã ã‘è¨˜äº‹åŒ–ï¼ˆé‡ã„å‡¦ç†ã‚’åŒæœŸçš„ã«å®Ÿè¡Œãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¶ç”¨ï¼‰"""
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_new_rss_articles
    news = fetch_rss_news()
    if not news:
        return {"status": "error", "article_id": None, "message": "RSSã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ¼ãƒ‰URLã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}
    added = process_new_rss_articles(news, max_per_run=1)
    if added <= 0:
        return {"status": "none", "article_id": None, "message": "å–ã‚Šè¾¼ã‚ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“"}
    NewsAggregator.get_news(force_refresh=True)
    updated = NewsAggregator.get_news()
    new_id = updated[0].id if updated else None
    return {"status": "ok", "article_id": new_id}


@router.get("/api/article/seed-one")
async def api_seed_one_article():
    """RSSã‹ã‚‰1ä»¶èª­ã¿è¾¼ã¿ã€AIè§£èª¬ä»˜ãã§è¨˜äº‹ã‚’1ä»¶ä½œã‚‹ã€‚ä½œæˆã—ãŸè¨˜äº‹IDã‚’è¿”ã™ï¼ˆå¸¸ã«JSONã§è¿”ã™ï¼‰"""
    if is_rss_and_ai_disabled():
        raise HTTPException(status_code=503, detail="ã“ã®ç’°å¢ƒã§ã¯RSSå–å¾—ãƒ»AIè¦ç´„ã¯ç„¡åŠ¹ã§ã™ã€‚")
    import asyncio
    import logging
    logger = logging.getLogger(__name__)
    try:
        loop = asyncio.get_event_loop()
        result = await asyncio.wait_for(
            loop.run_in_executor(None, _do_seed_one_sync),
            timeout=180.0,
        )
        return result
    except asyncio.TimeoutError:
        logger.warning("seed-one ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"status": "error", "article_id": None, "message": "å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ3åˆ†ï¼‰ã€‚RSSã‚„OpenAIã®å¿œç­”ãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"}
    except Exception as e:
        logger.exception("seed-one ã§ã‚¨ãƒ©ãƒ¼")
        msg = str(e).strip() or "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
        return {"status": "error", "article_id": None, "message": f"å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {msg}"}


def _do_force_add_one_sync():
    """RSSå–å¾—ï¼‹ãƒˆãƒ¬ãƒ³ãƒ‰ç²¾æŸ»ã§1ä»¶é¸å®šï¼‹AIå‡¦ç†ï¼ˆé‡ã„å‡¦ç†ã‚’åŒæœŸçš„ã«å®Ÿè¡Œãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¶ç”¨ï¼‰"""
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_rss_to_site_article, _rank_by_trending
    from app.services.explanation_cache import delete_cache
    from app.services.trends_service import fetch_trending_searches
    news = fetch_rss_news()
    if not news:
        return {"status": "error", "article_id": None, "message": "RSSã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ¼ãƒ‰URLã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}
    trends = fetch_trending_searches()
    trend_keywords = [t.keyword for t in trends]
    ranked = _rank_by_trending(news, trend_keywords) if trend_keywords else news
    item = ranked[0]
    delete_cache(item.id)
    if process_rss_to_site_article(item, force=True):
        NewsAggregator.get_news(force_refresh=True)
        if NewsAggregator.get_article(item.id) is None:
            return {"status": "error", "article_id": None, "message": "è¨˜äº‹ã®ä¿å­˜å¾Œã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚data ãƒ•ã‚©ãƒ«ãƒ€ã®æ¨©é™ã‚„DBã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}
        return {"status": "ok", "article_id": item.id}
    return {"status": "error", "article_id": None, "message": "AIè§£èª¬ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚.env ã® OPENAI_API_KEY ã‚’ç¢ºèªã—ã€åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆOPENAI_MODELï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"}


@router.get("/api/article/force-add-one")
async def api_force_add_one_article():
    """RSSã®å…ˆé ­1ä»¶ã‚’å¿…ãš1ä»¶å–ã‚Šè¾¼ã‚€ã€‚é‡ã„å‡¦ç†ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã—ã¦å›ºã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚"""
    if is_rss_and_ai_disabled():
        raise HTTPException(status_code=503, detail="ã“ã®ç’°å¢ƒã§ã¯RSSå–å¾—ãƒ»AIè¦ç´„ã¯ç„¡åŠ¹ã§ã™ã€‚")
    import asyncio
    import logging
    logger = logging.getLogger(__name__)
    try:
        loop = asyncio.get_event_loop()
        result = await asyncio.wait_for(
            loop.run_in_executor(None, _do_force_add_one_sync),
            timeout=180.0,
        )
        return result
    except asyncio.TimeoutError:
        logger.warning("force-add-one ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"status": "error", "article_id": None, "message": "å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ3åˆ†ï¼‰ã€‚RSSã‚„OpenAIã®å¿œç­”ãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"}
    except Exception as e:
        logger.exception("force-add-one ã§ã‚¨ãƒ©ãƒ¼")
        msg = str(e).strip() or "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
        return {"status": "error", "article_id": None, "message": f"å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {msg}"}

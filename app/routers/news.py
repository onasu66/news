"""ãƒ‹ãƒ¥ãƒ¼ã‚¹é–¢é€£ãƒ«ãƒ¼ãƒˆ"""
import uuid
from datetime import datetime

from fastapi import APIRouter, Request, HTTPException, Form, Header
from fastapi.responses import HTMLResponse, RedirectResponse, Response
from fastapi.templating import Jinja2Templates
from pathlib import Path

from app.config import settings
from app.services.news_aggregator import NewsAggregator
from app.services.rss_service import NewsItem, sanitize_display_text
from app.services.article_cache import save_article
from app.services.ai_batch_service import generate_all_explanations
from app.services.ai_service import (
    explain_article_with_ai,
    get_image_url,
    PERSONAS,
)

router = APIRouter()
templates = Jinja2Templates(directory=str(Path(__file__).resolve().parent.parent / "templates"))


@router.get("/robots.txt")
async def robots_txt(request: Request):
    """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³å‘ã‘ robots.txt"""
    site_url = _get_site_url(request)
    body = f"User-agent: *\nAllow: /\nDisallow: /admin\nDisallow: /confirm\n\nSitemap: {site_url}/sitemap.xml\n"
    return Response(content=body, media_type="text/plain; charset=utf-8")


@router.get("/sitemap.xml")
async def sitemap_xml(request: Request):
    """SEOç”¨ sitemap.xml"""
    from app.services.article_cache import load_all
    from app.services.explanation_cache import get_cached_article_ids

    site_url = _get_site_url(request)
    all_articles = load_all()
    processed = get_cached_article_ids()
    articles = [a for a in all_articles if a.id in processed]
    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">',
        f"  <url><loc>{site_url}/</loc><changefreq>hourly</changefreq><priority>1.0</priority></url>",
    ]
    for a in articles[:5000]:
        lines.append(f"  <url><loc>{site_url}/topic/{a.id}</loc><changefreq>weekly</changefreq><priority>0.8</priority></url>")
    lines.append("</urlset>")
    return Response(content="\n".join(lines), media_type="application/xml; charset=utf-8")


@router.get("/", response_class=HTMLResponse)
async def index(request: Request, page: int = 1):
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«åˆ¥è¡¨ç¤ºãƒ»ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""
    news_by_category, pagination = NewsAggregator.get_news_by_category(page=page)
    trends = NewsAggregator.get_trends()
    added_one = None
    if page == 1:
        all_news = NewsAggregator.get_news()
        if all_news:
            added_one = all_news[0]
            if added_one:
                if not added_one.image_url:
                    added_one.image_url = get_image_url(added_one.id, 400, 225)
                elif not added_one.image_url.startswith("http"):
                    added_one.image_url = get_image_url(added_one.image_url, 400, 225)
    for _, items in news_by_category:
        for item in items:
            if not item.image_url:
                item.image_url = get_image_url(item.id, 400, 225)
            elif item.image_url and not item.image_url.startswith("http"):
                item.image_url = get_image_url(item.image_url, 400, 225)
    site_url = _get_site_url(request)
    og_image = (added_one.image_url or "https://picsum.photos/1200/630") if added_one else "https://picsum.photos/1200/630"
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "news_by_category": news_by_category,
            "trends": trends,
            "pagination": pagination,
            "added_one": added_one,
            "site_url": site_url,
            "og_image": og_image,
        }
    )


# ç¢ºèªç”¨ãƒšãƒ¼ã‚¸ã§è¡¨ç¤ºã™ã‚‹ç›´è¿‘ã®è¨˜äº‹æ•°
CONFIRM_PAGE_ARTICLE_LIMIT = 20


@router.get("/confirm", response_class=HTMLResponse)
async def confirm_page(request: Request):
    """ç¢ºèªç”¨ãƒšãƒ¼ã‚¸ï¼šæœ€æ–°è¨˜äº‹ã®1ä»¶å–ã‚Šè¾¼ã¿ã¨ã€è¨˜äº‹ã®å‰Šé™¤"""
    news = NewsAggregator.get_news()[:CONFIRM_PAGE_ARTICLE_LIMIT]
    for item in news:
        if not item.image_url:
            item.image_url = get_image_url(item.id, 400, 225)
        elif item.image_url and not item.image_url.startswith("http"):
            item.image_url = get_image_url(item.image_url, 400, 225)
    return templates.TemplateResponse(
        "confirm.html",
        {"request": request, "recent_articles": news}
    )


def _get_site_url(request: Request) -> str:
    """ã‚µã‚¤ãƒˆã®çµ¶å¯¾URLï¼ˆæœ«å°¾ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã—ï¼‰"""
    base = getattr(settings, "SITE_URL", "").strip().rstrip("/")
    if base:
        return base
    return str(request.base_url).rstrip("/")


def _meta_description_qa(title: str, summary: str | None, max_len: int = 160) -> str:
    """è³ªå•ï¼‹è§£ç­”å‹ã®meta descriptionï¼ˆSEOå‘ã‘ï¼‰"""
    t = (title or "").strip()
    s = (summary or "").replace("\n", " ").strip()[:200]
    if not t:
        return (s[: max_len - 3] + "...") if len(s) > max_len else s
    question = f"{t}ã¨ã¯ï¼Ÿ"
    if not s:
        return question[:max_len]
    answer = s[: max_len - len(question) - 4] + "..." if len(s) > max_len - len(question) - 2 else s
    return f"{question} {answer}"[:max_len]


def _blocks_to_html(blocks: list) -> str:
    """ãƒ–ãƒ­ãƒƒã‚¯ã‚’HTMLã«å¤‰æ›ï¼ˆSSRç”¨ãƒ»XSSå¯¾ç­–æ¸ˆã¿ï¼‰"""
    if not blocks:
        return ""
    import html
    out = []
    is_navigator = blocks and blocks[0].get("type") == "navigator_section"
    nav_labels = {"facts": "ãƒ‹ãƒ¥ãƒ¼ã‚¹", "background": "èƒŒæ™¯", "impact": "å½±éŸ¿ç¯„å›²", "prediction": "äºˆæ¸¬", "caution": "æ³¨æ„"}
    if is_navigator:
        paras, asides = [], []
        for b in blocks:
            if b.get("type") != "navigator_section" or not b.get("section"):
                continue
            body = (b.get("content") or "").strip()
            body_safe = html.escape(body).replace("\n", "<br>") if body else ""
            if b.get("section") == "facts" and body:
                paras = body.split("\n\n")
                paras = [p.strip() for p in paras if p.strip()]
            elif b.get("section") != "facts" and body:
                asides.append({"section": b["section"], "label": nav_labels.get(b["section"], b["section"]), "body": body_safe})
        for i, p in enumerate(paras):
            out.append(f'<p class="article-text">{html.escape(p).replace("\n", "<br>")}</p>')
            if i < len(asides):
                a = asides[i]
                out.append(f'<div class="scroll-bubble-group"><div class="midorman-bubble-wrap is-visible"><div class="midorman-aside midorman-aside-{html.escape(a["section"])}"><span class="midorman-aside-label">{html.escape(a["label"])}</span><div class="midorman-aside-body">{a["body"]}</div></div></div></div>')
        for j in range(len(paras), len(asides)):
            a = asides[j]
            out.append(f'<div class="scroll-bubble-group"><div class="midorman-bubble-wrap is-visible"><div class="midorman-aside midorman-aside-{html.escape(a["section"])}"><span class="midorman-aside-label">{html.escape(a["label"])}</span><div class="midorman-aside-body">{a["body"]}</div></div></div></div>')
        return '<div class="article-readflow">' + "".join(out) + "</div>" if out else ""
    # text/explain å½¢å¼ï¼ˆH2/H3 ç–‘å•å‹ï¼‹å›ç­”å‹ã§SEOãƒ»ãƒŸãƒ‰ãƒ«ãƒãƒ³è§£èª¬ã‚‚æœ¬æ–‡ã«æ®‹ã™ï¼‰
    html_parts = ['<div class="article-readflow article-with-bubbles">']
    explain_index = 0
    for b in blocks:
        if b.get("type") == "explain":
            explain_index += 1
            c = html.escape(b.get("content") or "").replace("\n", "<br>")
            h3_id = f"explain-{explain_index}"
            html_parts.append(f'<h3 class="article-h3" id="{h3_id}">è¦ç‚¹ã®è§£èª¬</h3>')
            bubble = f'<div class="midorman-bubble-above"><span class="midorman-bubble-avatar" aria-hidden="true">ğŸ™ï¸</span><div class="midorman-bubble-inner"><p class="midorman-bubble-text">{c}</p></div></div>'
            html_parts.append(f'<div class="scroll-bubble-group"><div class="scroll-trigger" aria-hidden="true"></div><div class="midorman-bubble-wrap">{bubble}</div></div>')
        elif b.get("type") == "text":
            for p in (b.get("content") or "").strip().split("\n\n"):
                p = p.strip()
                if p:
                    html_parts.append(f'<p class="article-text">{html.escape(p).replace("\n", "<br>")}</p>')
    html_parts.append("</div>")
    return "".join(html_parts)


@router.get("/topic/{topic_id}", response_class=HTMLResponse)
async def topic_detail(request: Request, topic_id: str):
    """ãƒˆãƒ”ãƒƒã‚¯è©³ç´°ï¼ˆURL: /topic/â—‹â—‹ï¼‰ãƒ»AIè§£èª¬ãƒ»SEOå‘ã‘æœ¬æ–‡"""
    from app.services.explanation_cache import get_cached

    item = NewsAggregator.get_article(topic_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    image_url = item.image_url or get_image_url(item.id, 800, 450)
    if image_url and not image_url.startswith("http"):
        image_url = get_image_url(image_url, 800, 450)
    site_url = _get_site_url(request)
    article_url = f"{site_url}/topic/{topic_id}"
    og_image = image_url if (image_url or "").startswith("http") else f"{site_url}{image_url}" if image_url else ""
    if not og_image:
        og_image = get_image_url(item.id, 1200, 630)
    cached = get_cached(topic_id)
    blocks = _sanitize_blocks(cached["blocks"]) if cached and cached.get("blocks") else []
    personas_data = cached.get("personas", []) if cached else []
    body_html = _blocks_to_html(blocks) if blocks else ""
    meta_desc = _meta_description_qa(item.title, item.summary)

    all_news = NewsAggregator.get_news()
    next_article = prev_article = None
    for i, a in enumerate(all_news):
        if a.id == topic_id:
            if i + 1 < len(all_news):
                next_article = all_news[i + 1]
            if i > 0:
                prev_article = all_news[i - 1]
            break

    related = [a for a in all_news if a.category == item.category and a.id != topic_id][:4]

    return templates.TemplateResponse(
        "article.html",
        {
            "request": request,
            "article": item,
            "image_url": image_url,
            "personas": PERSONAS,
            "site_url": site_url,
            "article_url": article_url,
            "og_image": og_image,
            "blocks": blocks,
            "personas_data": personas_data,
            "body_html": body_html,
            "meta_description": meta_desc,
            "next_article": next_article,
            "prev_article": prev_article,
            "related_articles": related,
        }
    )


@router.get("/article/{article_id}", response_class=HTMLResponse)
async def article_detail(request: Request, article_id: str):
    """æ—§URL: /topic/ ã¸ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    from fastapi.responses import RedirectResponse
    return RedirectResponse(url=f"/topic/{article_id}", status_code=301)


def _sanitize_blocks(blocks: list) -> list:
    """ãƒ–ãƒ­ãƒƒã‚¯ã®contentã‹ã‚‰HTMLæ–­ç‰‡ã‚’é™¤å»ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿æ‚ªãƒ‡ãƒ¼ã‚¿å¯¾ç­–ï¼‰"""
    out = []
    for b in (blocks or []):
        if isinstance(b, dict) and "content" in b and b.get("content"):
            b = {**b, "content": sanitize_display_text(str(b["content"]))}
        out.append(b)
    return out


@router.get("/api/article/{article_id}/explain")
async def api_explain_article(article_id: str):
    """è¨˜äº‹ã®AIè§£èª¬ã‚’å–å¾—ï¼ˆå¾“æ¥å½¢å¼ãƒ»ã‚µã‚¤ãƒ‰ãƒ‘ãƒãƒ«ç”¨ï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    content = f"{item.title}\n\n{item.summary}"
    explanation = explain_article_with_ai(item.title, content)
    return {"explanation": explanation}


@router.get("/api/article/{article_id}/explain-inline")
async def api_explain_inline(article_id: str):
    """è¨˜äº‹æœ¬æ–‡ã¨è§£èª¬ãŒäº¤äº’ã«å…¥ã£ãŸæ§‹é€ ã§å–å¾—ï¼ˆå¾“æ¥APIãƒ»äº’æ›ç”¨ï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    data = generate_all_explanations(article_id, item.title, f"{item.title}\n\n{item.summary}")
    return {"blocks": _sanitize_blocks(data["blocks"])}


@router.get("/api/article/{article_id}/explanations")
async def api_all_explanations(article_id: str):
    """ãƒŸãƒ‰ãƒ«ãƒãƒ³è§£èª¬ï¼‹5äººæ ¼ã®æ„è¦‹ã‚’ä¸€æ‹¬å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆãƒ»ä¸€æ‹¬ç”Ÿæˆï¼‰"""
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    content = f"{item.title}\n\n{item.summary}"
    data = generate_all_explanations(article_id, item.title, content)
    return {"blocks": _sanitize_blocks(data["blocks"]), "personas": data["personas"]}


@router.get("/api/article/{article_id}/opinion/{persona_id}")
async def api_persona_opinion(article_id: str, persona_id: int):
    """5äººæ ¼ã®AIã®ã†ã¡1äººã®æ„è¦‹ã‚’å–å¾—ï¼ˆå¾“æ¥APIãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµŒç”±ï¼‰"""
    if persona_id < 0 or persona_id >= len(PERSONAS):
        raise HTTPException(status_code=404, detail="äººæ ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    item = NewsAggregator.get_article(article_id)
    if not item:
        raise HTTPException(status_code=404, detail="è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    data = generate_all_explanations(article_id, item.title, f"{item.title}\n\n{item.summary}")
    opinion = data["personas"][persona_id] if persona_id < len(data["personas"]) else ""
    return {"persona": PERSONAS[persona_id], "opinion": opinion}


@router.get("/api/status")
async def api_status():
    """çŠ¶æ…‹ç¢ºèªï¼ˆè¨˜äº‹æ•°ãƒ»DBãƒ‘ã‚¹ç­‰ï¼‰"""
    from app.services.explanation_cache import get_cached_article_ids
    from app.services.article_cache import load_all

    processed = get_cached_article_ids()
    all_arts = load_all()
    displayable = [a for a in all_arts if a.id in processed]
    try:
        from app.config import settings
        has_key = bool(getattr(settings, "OPENAI_API_KEY", ""))
    except Exception:
        has_key = False

    return {
        "articles_in_db": len(all_arts),
        "ai_processed": len(processed),
        "displayable": len(displayable),
        "openai_key_set": has_key,
    }


@router.get("/api/news/refresh")
async def api_refresh_news():
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’æ‰‹å‹•æ›´æ–°ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã€å³å¿œç­”ï¼‰"""
    import threading

    def _refresh():
        NewsAggregator.get_trends(force_refresh=True)  # ãƒˆãƒ¬ãƒ³ãƒ‰ã¯é€Ÿã„
        NewsAggregator.get_news(force_refresh=True)   # è¨˜äº‹ã¯AIå‡¦ç†ã§é…ã„

    threading.Thread(target=_refresh, daemon=True).start()
    return {"status": "ok", "message": "æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã—ãŸ"}


def _is_admin(request: Request, x_admin_secret: str | None = Header(None, alias="X-Admin-Secret")) -> bool:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¾ãŸã¯ X-Admin-Secret ãƒ˜ãƒƒãƒ€ã§ç®¡ç†è€…ã‹åˆ¤å®š"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        return False
    if x_admin_secret and x_admin_secret.strip() == settings.ADMIN_SECRET:
        return True
    return request.session.get("admin") is True


@router.get("/admin/login", response_class=HTMLResponse)
async def admin_login_page(request: Request):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ã‚©ãƒ¼ãƒ è¡¨ç¤º"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        return templates.TemplateResponse("admin_login.html", {"request": request, "error": "ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ï¼ˆADMIN_SECRET æœªè¨­å®šï¼‰"})
    if request.session.get("admin"):
        return RedirectResponse(url="/admin", status_code=302)
    err = "ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“" if request.query_params.get("error") == "invalid" else None
    return templates.TemplateResponse("admin_login.html", {"request": request, "error": err})


@router.post("/admin/login", response_class=RedirectResponse)
async def admin_login_submit(request: Request, secret: str = Form(...)):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        raise HTTPException(status_code=403, detail="ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
    if secret.strip() != settings.ADMIN_SECRET:
        return RedirectResponse(url="/admin/login?error=invalid", status_code=302)
    request.session["admin"] = True
    return RedirectResponse(url="/admin", status_code=302)


@router.get("/admin/logout", response_class=RedirectResponse)
async def admin_logout(request: Request):
    """ç®¡ç†è€…ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"""
    request.session.pop("admin", None)
    return RedirectResponse(url="/", status_code=302)


@router.get("/admin", response_class=HTMLResponse)
async def admin_manual_article_page(request: Request):
    """æ‰‹å‹•è¨˜äº‹è¿½åŠ ãƒ•ã‚©ãƒ¼ãƒ ï¼ˆç®¡ç†è€…ã®ã¿ï¼‰"""
    if not getattr(settings, "ADMIN_SECRET", ""):
        raise HTTPException(status_code=403, detail="ç®¡ç†æ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™")
    if not request.session.get("admin"):
        return RedirectResponse(url="/admin/login", status_code=302)
    return templates.TemplateResponse("admin_manual_article.html", {"request": request})


def _do_create_manual_article_sync(title: str, summary: str, link: str = "", source: str = "ç·¨é›†éƒ¨") -> dict:
    """æ‰‹å‹•è¨˜äº‹ã‚’AIã§ç”Ÿæˆã—ã¦ä¿å­˜ï¼ˆåŒæœŸãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œç”¨ï¼‰ã€‚generate_all_explanations å†…ã§ save_cache æ¸ˆã¿"""
    article_id = "manual-" + uuid.uuid4().hex[:16]
    content = sanitize_display_text(f"{title}\n\n{summary}")[:20000]
    data = generate_all_explanations(article_id, title, content)
    blocks = data.get("blocks", [])
    if not blocks:
        return {"status": "error", "article_id": None, "message": "AIã«ã‚ˆã‚‹è¨˜äº‹ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ"}
    item = NewsItem(
        id=article_id,
        title=title,
        link=link or "#",
        summary=summary[:4000],
        published=datetime.now(),
        source=source or "ç·¨é›†éƒ¨",
        category="ç·åˆ",
        image_url=None,
    )
    if not save_article(item):
        return {"status": "error", "article_id": None, "message": "è¨˜äº‹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ"}
    NewsAggregator.get_news(force_refresh=True)
    return {"status": "ok", "article_id": article_id}


@router.post("/api/admin/article/manual")
async def api_admin_article_manual(
    request: Request,
    x_admin_secret: str | None = Header(None, alias="X-Admin-Secret"),
):
    """æ‰‹å‹•ã§æ¦‚è¦ã‚’é€ã‚Šã€AIãŒç†è§£ãƒŠãƒ“ã‚²ãƒ¼ã‚¿ãƒ¼å½¢å¼ã®è¨˜äº‹ã‚’ç”Ÿæˆã—ã¦è¿½åŠ ï¼ˆç®¡ç†è€…ã®ã¿ï¼‰"""
    if not _is_admin(request, x_admin_secret):
        raise HTTPException(status_code=403, detail="ç®¡ç†è€…ã®ã¿åˆ©ç”¨ã§ãã¾ã™")
    try:
        body = await request.json()
    except Exception:
        raise HTTPException(status_code=400, detail="JSON ã§ title, summary ã‚’é€ã£ã¦ãã ã•ã„")
    title = (body.get("title") or "").strip()
    summary = (body.get("summary") or "").strip()
    if not title or not summary:
        raise HTTPException(status_code=400, detail="ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã¯å¿…é ˆã§ã™")
    link = (body.get("link") or "").strip()
    source = (body.get("source") or "ç·¨é›†éƒ¨").strip() or "ç·¨é›†éƒ¨"
    import asyncio
    loop = asyncio.get_event_loop()
    result = await asyncio.wait_for(
        loop.run_in_executor(
            None,
            lambda: _do_create_manual_article_sync(title, summary, link, source),
        ),
        timeout=180.0,
    )
    return result


@router.post("/api/admin/article/{article_id}/clear-cache")
async def api_clear_article_cache(article_id: str):
    """è¨˜äº‹ã®è§£èª¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼ˆå†ç”Ÿæˆã•ã›ã‚‹ï¼‰"""
    from app.services.explanation_cache import delete_cache
    deleted = delete_cache(article_id)
    return {"status": "ok", "deleted": deleted, "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚æ¬¡å›ã‚¢ã‚¯ã‚»ã‚¹ã§å†ç”Ÿæˆã•ã‚Œã¾ã™ã€‚"}


@router.post("/api/admin/article/{article_id}/delete")
async def api_delete_article(article_id: str):
    """è¨˜äº‹ã‚’å®Œå…¨ã«å‰Šé™¤ï¼ˆè§£èª¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‹è¨˜äº‹DBã‹ã‚‰å‰Šé™¤ï¼‰"""
    from app.services.explanation_cache import delete_cache
    from app.services.article_cache import delete_article as delete_article_from_db
    deleted_cache = delete_cache(article_id)
    deleted_article = delete_article_from_db(article_id)
    NewsAggregator.get_news(force_refresh=True)
    return {
        "status": "ok",
        "deleted": deleted_cache or deleted_article,
        "message": "è¨˜äº‹ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚",
    }


@router.get("/api/admin/seed-articles")
async def api_seed_articles():
    """RSSã‹ã‚‰ãƒŸãƒ‰ãƒ«ãƒãƒ³AIè§£èª¬ä»˜ãã§è¨˜äº‹ã‚’æŠ•å…¥ï¼ˆæ–°ç€5ä»¶ï¼‰"""
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_new_rss_articles
    from app.services.explanation_cache import get_cached_article_ids

    news = fetch_rss_news()
    added = process_new_rss_articles(news, max_per_run=5)
    NewsAggregator.get_news(force_refresh=True)
    total = len(get_cached_article_ids())
    return {"status": "ok", "added": added, "total": total}


@router.get("/api/article/seed-one")
async def api_seed_one_article():
    """RSSã‹ã‚‰1ä»¶èª­ã¿è¾¼ã¿ã€AIè§£èª¬ä»˜ãã§è¨˜äº‹ã‚’1ä»¶ä½œã‚‹ã€‚ä½œæˆã—ãŸè¨˜äº‹IDã‚’è¿”ã™"""
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_new_rss_articles

    news = fetch_rss_news()
    added = process_new_rss_articles(news, max_per_run=1)
    if added <= 0:
        return {"status": "none", "article_id": None, "message": "å–ã‚Šè¾¼ã‚ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“"}
    NewsAggregator.get_news(force_refresh=True)
    # å…ˆé ­ï¼ä»Šè¿½åŠ ã—ãŸ1ä»¶
    updated = NewsAggregator.get_news()
    new_id = updated[0].id if updated else None
    return {"status": "ok", "article_id": new_id}


def _do_force_add_one_sync():
    """RSSå–å¾—ï¼‹ãƒˆãƒ¬ãƒ³ãƒ‰ç²¾æŸ»ã§1ä»¶é¸å®šï¼‹AIå‡¦ç†ï¼ˆé‡ã„å‡¦ç†ã‚’åŒæœŸçš„ã«å®Ÿè¡Œãƒ»ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¶ç”¨ï¼‰"""
    from app.services.rss_service import fetch_rss_news
    from app.services.article_processor import process_rss_to_site_article, _rank_by_trending
    from app.services.explanation_cache import delete_cache
    from app.services.trends_service import fetch_trending_searches
    news = fetch_rss_news()
    if not news:
        return {"status": "error", "article_id": None, "message": "RSSã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚£ãƒ¼ãƒ‰URLã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}
    trends = fetch_trending_searches()
    trend_keywords = [t.keyword for t in trends]
    ranked = _rank_by_trending(news, trend_keywords) if trend_keywords else news
    item = ranked[0]
    delete_cache(item.id)
    if process_rss_to_site_article(item, force=True):
        NewsAggregator.get_news(force_refresh=True)
        if NewsAggregator.get_article(item.id) is None:
            return {"status": "error", "article_id": None, "message": "è¨˜äº‹ã®ä¿å­˜å¾Œã«å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚data ãƒ•ã‚©ãƒ«ãƒ€ã®æ¨©é™ã‚„DBã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"}
        return {"status": "ok", "article_id": item.id}
    return {"status": "error", "article_id": None, "message": "AIè§£èª¬ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚.env ã® OPENAI_API_KEY ã‚’ç¢ºèªã—ã€åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆOPENAI_MODELï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"}


@router.get("/api/article/force-add-one")
async def api_force_add_one_article():
    """RSSã®å…ˆé ­1ä»¶ã‚’å¿…ãš1ä»¶å–ã‚Šè¾¼ã‚€ã€‚é‡ã„å‡¦ç†ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã—ã¦å›ºã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚"""
    import asyncio
    import logging
    logger = logging.getLogger(__name__)
    try:
        loop = asyncio.get_event_loop()
        result = await asyncio.wait_for(
            loop.run_in_executor(None, _do_force_add_one_sync),
            timeout=180.0,
        )
        return result
    except asyncio.TimeoutError:
        logger.warning("force-add-one ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
        return {"status": "error", "article_id": None, "message": "å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ3åˆ†ï¼‰ã€‚RSSã‚„OpenAIã®å¿œç­”ãŒé…ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"}
    except Exception as e:
        logger.exception("force-add-one ã§ã‚¨ãƒ©ãƒ¼")
        msg = str(e).strip() or "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
        return {"status": "error", "article_id": None, "message": f"å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {msg}"}
